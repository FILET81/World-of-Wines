{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "#from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#from sklearn.preprocessing import LabelEncoder   #####\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#import category_encoders as ce  #####\n",
    "#from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WinesDatasetCleaning import wine_dataset_cleaning as wdc\n",
    "wines_dataset = wdc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns:\n",
    "wines_dataset.drop(columns=[\"country\",\n",
    "                            \"apellation\",\n",
    "                            \"taster_name\",\n",
    "                            \"taster_twitter_handle\",\n",
    "                            \"title\",\n",
    "                            \"variety\",\n",
    "                            \"winery\",\n",
    "                            \"noble_international\",\n",
    "                            \"monovarietal\",\n",
    "                            \"taste_alcohol\",\n",
    "                            \"primary_flavors\",\n",
    "                            \"title_new\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wines_filtered = wines_dataset[wines_dataset[\"price_usd\"]<=75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = np.percentile(wines_dataset[\"vintage\"], 75)-np.percentile(wines_dataset[\"vintage\"], 25)\n",
    "lower_limit = np.percentile(wines_dataset[\"vintage\"], 25)-3*iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_predictor_v0 = wines_dataset[wines_dataset[\"vintage\"] >= lower_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dependent variable:\n",
    "y_v0 = wines_predictor_v0[\"price_usd\"]\n",
    "wines_predictor_v0.drop(columns=[\"price_usd\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables: \n",
    "X_num_v0 = wines_predictor_v0.select_dtypes(np.number)     # It takes all \"numerical\" variables\n",
    "X_cat_v0 = wines_predictor_v0.select_dtypes(object)     # It takes all \"categorical\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \"numerical\" variables:\n",
    "scaler_num_v0 = MinMaxScaler().fit_transform(X_num_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for \"categorical\" variables:\n",
    "dummies_cat_v0 = pd.get_dummies(X_cat_v0, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating both \"X_num\" and \"X_cat\" variables:\n",
    "X_v0 = np.concatenate((scaler_num_v0, dummies_cat_v0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data between \"train\" and \"test\":\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_v0, y_v0, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Simple)_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Linear Regression (Simple):\n",
      "MSE (Mean Squared Error): 1499.2795\n",
      "R2 (Coefficient of Determination): 0.2004\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fitting the model:\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lm = lm.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "r2_lm = r2_score(y_test, y_pred_lm)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"ðŸ“˜ Linear Regression (Simple):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lm:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Ridge)_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Ridge's Best Parameters: {'alpha': 0.1, 'max_iter': 1000, 'solver': 'sag'}\n",
      "Ridge's Best Score: 0.20778486531774454\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"solver\": [\"auto\", \"sag\", \"lsqr\"],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=Ridge(),     # Model we want to fit\n",
    "    param_grid=param_grid_ridge,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Ridge's Best Parameters:\", grid_search_ridge.best_params_)\n",
    "print(\"Ridge's Best Score:\", grid_search_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Linear Regression (Ridge):\n",
      "MSE (Mean Squared Error): 1499.2794\n",
      "R2 (Coefficient of Determination): 0.2004\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "ridge = Ridge(alpha=0.1, max_iter=10000, solver=\"sag\")\n",
    "\n",
    "# Fitting the model:\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"ðŸ“˜ Linear Regression (Ridge):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_ridge:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Lasso)_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Lasso's Best Parameters: {'alpha': 0.0001, 'max_iter': 1000, 'tol': 1e-06}\n",
      "Lasso's Best Score: 0.20778439478417782\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_lasso = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"tol\": [1e-4, 1e-5, 1e-6],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=Lasso(),     # Model we want to fit\n",
    "    param_grid=param_grid_lasso,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Lasso's Best Parameters:\", grid_search_lasso.best_params_)\n",
    "print(\"Lasso's Best Score:\", grid_search_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Linear Regression (Lasso):\n",
      "MSE (Mean Squared Error): 1499.2809\n",
      "R2 (Coefficient of Determination): 0.2004\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "lasso = Lasso(alpha=0.0001, max_iter=1000, tol=0.000001)\n",
    "\n",
    "# Fitting the model:\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"ðŸ“˜ Linear Regression (Lasso):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lasso:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    }
   ],
   "source": [
    "\"\"\" # Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_svr = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"epsilon\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"kernel\": [\"rbf\", \"linear\", \"sigmoid\"]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_svr = GridSearchCV(\n",
    "    estimator=SVR(),     # Model we want to fit\n",
    "    param_grid=param_grid_svr,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_svr.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"SVR's Best Parameters:\", grid_search_svr.best_params_)\n",
    "print(\"SVR's Best Score:\", grid_search_svr.best_score_) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Support Vector Regression:\n",
      "MSE (Mean Squared Error): 1421.2532\n",
      "R2 (Coefficient of Determination): 0.2420\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "svr = SVR(C=100, epsilon=0.5, kernel=\"rbf\")\n",
    "\n",
    "# Fitting the model:\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"ðŸ“˜ Support Vector Regression:\")\n",
    "print(f'MSE (Mean Squared Error): {mse_svr:.4f}')\n",
    "print(f'R2 (Coefficient of Determination): {r2_svr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_rfr = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_rfr = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),     # Model we want to fit\n",
    "    param_grid=param_grid_rfr,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_rfr.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"RFR's Best Parameters:\", grid_search_rfr.best_params_)\n",
    "print(\"RFR's Best Score:\", grid_search_rfr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Random Forest Regression:\n",
      "MSE (Mean Squared Error): 140.3977\n",
      "R2 (Coefficient of Determination): 0.4621\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "rfr = RandomForestRegressor(n_estimators=100, random_state=42, min_samples_leaf=4, min_samples_split=5, bootstrap=True, max_depth=20, max_features=\"sqrt\")       # Original -> n_estimators=100, random_state=42\n",
    "# n_estimators=100, random_state=42, min_samples_leaf=4, min_samples_split=5, bootstrap=True, max_depth=20, max_features=\"sqrt\"\n",
    "\n",
    "# Fitting the model:\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_rfr = rfr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_rfr = mean_squared_error(y_test, y_pred_rfr)\n",
    "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"ðŸ“˜ Random Forest Regression:\")\n",
    "print(f'MSE (Mean Squared Error): {mse_rfr:.4f}')\n",
    "print(f'R2 (Coefficient of Determination): {r2_rfr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Original",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

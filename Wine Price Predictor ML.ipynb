{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WinesDatasetCleaning import wine_dataset_cleaning as wdc\n",
    "wines_dataset = wdc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns:\n",
    "wines_dataset.drop(columns=[\"country\",\n",
    "                            \"apellation\",\n",
    "                            \"taster_name\",\n",
    "                            \"taster_twitter_handle\",\n",
    "                            \"title\",\n",
    "                            \"variety\",\n",
    "                            \"winery\",\n",
    "                            \"noble_international\",\n",
    "                            \"monovarietal\",\n",
    "                            \"taste_alcohol\",\n",
    "                            \"primary_flavors\",\n",
    "                            \"title_new\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr = np.percentile(wines_dataset[\"vintage\"], 75)-np.percentile(wines_dataset[\"vintage\"], 25)\n",
    "lower_limit = np.percentile(wines_dataset[\"vintage\"], 25)-3*iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_predictor_v0 = wines_dataset[wines_dataset[\"vintage\"] >= lower_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dependent variable:\n",
    "y_v0 = wines_predictor_v0[\"price_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables:\n",
    "X_v0 = wines_predictor_v0.drop(columns=[\"price_usd\"])\n",
    "\n",
    "X_num_v0 = X_v0.select_dtypes(np.number)     # It takes all \"numerical\" variables\n",
    "X_cat_v0 = X_v0.select_dtypes(object)     # It takes all \"categorical\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \"numerical\" variables:\n",
    "scaler_num_v0 = MinMaxScaler().fit_transform(X_num_v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for \"categorical\" variables:\n",
    "dummies_cat_v0 = pd.get_dummies(X_cat_v0, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating both \"X_num\" and \"X_cat\" variables:\n",
    "X_v0 = np.concatenate((scaler_num_v0, dummies_cat_v0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data between \"train\" and \"test\":\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_v0, y_v0, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Simple)_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Simple):\n",
      "MSE (Mean Squared Error): 1499.2795\n",
      "R2 (Coefficient of Determination): 0.2004\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fitting the model:\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lm = lm.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "r2_lm = r2_score(y_test, y_pred_lm)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Simple):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lm:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Ridge)_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Ridge's Best Parameters: {'alpha': 0.1, 'max_iter': 5000, 'solver': 'sag'}\n",
      "Ridge's Best Score: 0.20778502764636383\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"solver\": [\"auto\", \"sag\", \"lsqr\"],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=Ridge(),     # Model we want to fit\n",
    "    param_grid=param_grid_ridge,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Ridge's Best Parameters:\", grid_search_ridge.best_params_)\n",
    "print(\"Ridge's Best Score:\", grid_search_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Ridge):\n",
      "MSE (Mean Squared Error): 1499.2934\n",
      "R2 (Coefficient of Determination): 0.2004\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "ridge = Ridge(alpha=1, max_iter=1000, solver=\"sag\")\n",
    "\n",
    "# Fitting the model:\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Ridge):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_ridge:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Lasso)_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Lasso's Best Parameters: {'alpha': 0.0001, 'max_iter': 1000, 'tol': 1e-06}\n",
      "Lasso's Best Score: 0.20778439478417782\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_lasso = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"tol\": [1e-4, 1e-5, 1e-6],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=Lasso(),     # Model we want to fit\n",
    "    param_grid=param_grid_lasso,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Lasso's Best Parameters:\", grid_search_lasso.best_params_)\n",
    "print(\"Lasso's Best Score:\", grid_search_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Lasso):\n",
      "MSE (Mean Squared Error): 1499.2809\n",
      "R2 (Coefficient of Determination): 0.2004\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "lasso = Lasso(alpha=0.0001, max_iter=1000, tol=0.000001)\n",
    "\n",
    "# Fitting the model:\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Lasso):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lasso:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Defining the parameter grid (check the model\\'s documentation to know the exact names of the hyperparameters):\\nparam_grid_svr = {\\n    \"C\": [0.1, 1, 10, 100],\\n    \"epsilon\": [0.001, 0.01, 0.1, 0.5],\\n    \"kernel\": [\"rbf\", \"linear\", \"sigmoid\"]}\\n\\n# Creating GridSearchCV:\\ngrid_search_svr = GridSearchCV(\\n    estimator=SVR(),     # Model we want to fit\\n    param_grid=param_grid_svr,     # Hyperparameters grid\\n    cv=5,     # Number of Cross-Validation folds\\n    scoring=\"r2\",     # Scoring method we\\'ll use to determine the best hyperparameters\\n\\tverbose=1)     # It prints the results at each step\\n\\n# Performing Grid Search:\\ngrid_search_svr.fit(X_train, y_train)\\n\\n# Returning Best parameters & Best score:\\nprint(\"SVR\\'s Best Parameters:\", grid_search_svr.best_params_)\\nprint(\"SVR\\'s Best Score:\", grid_search_svr.best_score_) '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_svr = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"epsilon\": [0.001, 0.01, 0.1, 0.5],\n",
    "    \"kernel\": [\"rbf\", \"linear\", \"sigmoid\"]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_svr = GridSearchCV(\n",
    "    estimator=SVR(),     # Model we want to fit\n",
    "    param_grid=param_grid_svr,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_svr.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"SVR's Best Parameters:\", grid_search_svr.best_params_)\n",
    "print(\"SVR's Best Score:\", grid_search_svr.best_score_) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Support Vector Regression:\n",
      "MSE (Mean Squared Error): 1421.2532\n",
      "R2 (Coefficient of Determination): 0.2420\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "svr = SVR(C=100, epsilon=0.5, kernel=\"rbf\")\n",
    "\n",
    "# Fitting the model:\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Support Vector Regression:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_svr:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_svr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Defining the parameter grid (check the model\\'s documentation to know the exact names of the hyperparameters):\\nparam_grid_rfr = {\\n    \"n_estimators\": [100, 300, 500],\\n    \"max_depth\": [10, 20],\\n    \"min_samples_leaf\": [1, 2, 4]}\\n\\n# Creating GridSearchCV:\\ngrid_search_rfr = GridSearchCV(\\n    estimator=RandomForestRegressor(),     # Model we want to fit\\n    param_grid=param_grid_rfr,     # Hyperparameters grid\\n    cv=5,     # Number of Cross-Validation folds\\n    scoring=\"r2\",     # Scoring method we\\'ll use to determine the best hyperparameters\\n\\tverbose=1)     # It prints the results at each step\\n\\n# Performing Grid Search:\\ngrid_search_rfr.fit(X_train, y_train)\\n\\n# Returning Best parameters & Best score:\\nprint(\"RFR\\'s Best Parameters:\", grid_search_rfr.best_params_)\\nprint(\"RFR\\'s Best Score:\", grid_search_rfr.best_score_) '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_rfr = {\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    \"max_depth\": [10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_rfr = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),     # Model we want to fit\n",
    "    param_grid=param_grid_rfr,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_rfr.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"RFR's Best Parameters:\", grid_search_rfr.best_params_)\n",
    "print(\"RFR's Best Score:\", grid_search_rfr.best_score_) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Random Forest Regression:\n",
      "MSE (Mean Squared Error): 1262.1406\n",
      "R2 (Coefficient of Determination): 0.3269\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=4, random_state=42) \n",
    "\n",
    "# Fitting the model:\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_rfr = rfr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_rfr = mean_squared_error(y_test, y_pred_rfr)\n",
    "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Random Forest Regression:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_rfr:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_rfr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying VIF for Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_vif = wines_dataset.drop(columns=[\"wine_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iteración 1 ---\n",
      "           Variable          VIF\n",
      "0            points   856.068218\n",
      "1         avg_abv_%   261.263612\n",
      "2  avg_serve_temp_c    73.843123\n",
      "3   taste_dry-sweet    23.711092\n",
      "4        taste_body    52.478985\n",
      "5     taste_tannins    24.319699\n",
      "6     taste_acidity    39.716134\n",
      "7           vintage  1251.634174\n",
      "\n",
      "⚠️ Se eliminará la variable con mayor VIF: vintage (VIF=1251.6341736933716)\n",
      "\n",
      "--- Iteración 2 ---\n",
      "           Variable         VIF\n",
      "0            points  279.062722\n",
      "1         avg_abv_%  192.465820\n",
      "2  avg_serve_temp_c   73.788523\n",
      "3   taste_dry-sweet   22.483527\n",
      "4        taste_body   52.451080\n",
      "5     taste_tannins   24.289661\n",
      "6     taste_acidity   32.834957\n",
      "\n",
      "⚠️ Se eliminará la variable con mayor VIF: points (VIF=279.06272182856196)\n",
      "\n",
      "--- Iteración 3 ---\n",
      "           Variable        VIF\n",
      "0         avg_abv_%  69.958262\n",
      "1  avg_serve_temp_c  73.219334\n",
      "2   taste_dry-sweet  18.228289\n",
      "3        taste_body  52.401812\n",
      "4     taste_tannins  24.247313\n",
      "5     taste_acidity  16.141179\n",
      "\n",
      "⚠️ Se eliminará la variable con mayor VIF: avg_serve_temp_c (VIF=73.21933449199145)\n",
      "\n",
      "--- Iteración 4 ---\n",
      "          Variable        VIF\n",
      "0        avg_abv_%  61.823504\n",
      "1  taste_dry-sweet  18.133604\n",
      "2       taste_body  46.130724\n",
      "3    taste_tannins  16.272873\n",
      "4    taste_acidity  16.090743\n",
      "\n",
      "⚠️ Se eliminará la variable con mayor VIF: avg_abv_% (VIF=61.823504116502384)\n",
      "\n",
      "--- Iteración 5 ---\n",
      "          Variable        VIF\n",
      "0  taste_dry-sweet  14.143118\n",
      "1       taste_body  25.674658\n",
      "2    taste_tannins  16.028303\n",
      "3    taste_acidity  12.068567\n",
      "\n",
      "⚠️ Se eliminará la variable con mayor VIF: taste_body (VIF=25.6746575850995)\n",
      "\n",
      "--- Iteración 6 ---\n",
      "          Variable        VIF\n",
      "0  taste_dry-sweet  10.576649\n",
      "1    taste_tannins   3.120896\n",
      "2    taste_acidity  11.988571\n",
      "\n",
      "⚠️ Se eliminará la variable con mayor VIF: taste_acidity (VIF=11.98857135194368)\n",
      "\n",
      "--- Iteración 7 ---\n",
      "          Variable       VIF\n",
      "0  taste_dry-sweet  2.736824\n",
      "1    taste_tannins  2.736824\n",
      "\n",
      "✅ Todas las variables tienen VIF menor que 10\n",
      "\n",
      "📊 Proceso finalizado. Variables restantes:\n",
      "Index(['taste_dry-sweet', 'taste_tannins'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def calcular_vif(wines_vif):\n",
    "    \"\"\"Calcula el VIF para cada variable en el DataFrame.\"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = wines_vif.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(wines_vif.values, i) for i in range(wines_vif.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "def eliminar_variables_vif(wines_vif, umbral=10):\n",
    "    \"\"\"\n",
    "    Elimina las variables con el mayor VIF de forma iterativa, \n",
    "    hasta que todas las variables tengan un VIF inferior al umbral.\n",
    "    \"\"\"\n",
    "    iteracion = 1\n",
    "    while True:\n",
    "        print(f\"\\n--- Iteración {iteracion} ---\")\n",
    "        \n",
    "\n",
    "        vif_data = calcular_vif(wines_vif)\n",
    "        print(vif_data)\n",
    "        \n",
    "        max_vif = vif_data[\"VIF\"].max()\n",
    "        if max_vif < umbral:\n",
    "            print(\"\\n✅ Todas las variables tienen VIF menor que\", umbral)\n",
    "            break\n",
    "        \n",
    "\n",
    "        variable_a_eliminar = vif_data.loc[vif_data[\"VIF\"].idxmax(), \"Variable\"]\n",
    "        print(f\"\\n⚠️ Se eliminará la variable con mayor VIF: {variable_a_eliminar} (VIF={max_vif})\")\n",
    "        \n",
    "        wines_vif = wines_vif.drop(columns=[variable_a_eliminar])\n",
    "        \n",
    "        iteracion += 1\n",
    "        \n",
    "    print(\"\\n📊 Proceso finalizado. Variables restantes:\")\n",
    "    print(wines_vif.columns)\n",
    "    return wines_vif, vif_data\n",
    "\n",
    "data_final, vif_final = eliminar_variables_vif(wines_vif.drop([\"price_usd\"], axis=1), umbral=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dependent variable:\n",
    "y_v2 = wines_predictor_v0[\"price_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables according to VIF:\n",
    "X_v2 = wines_predictor_v0.drop(columns=[\"price_usd\", \"points\", \"avg_abv_%\", \"avg_serve_temp_c\", \"taste_body\", \"taste_acidity\", \"vintage\"])\n",
    "\n",
    "X_num_v2 = X_v2.select_dtypes(np.number)     # It takes all \"numerical\" variables\n",
    "X_cat_v2 = X_v2.select_dtypes(object)     # It takes all \"categorical\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \"numerical\" variables:\n",
    "scaler_num_v2 = MinMaxScaler().fit_transform(X_num_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for \"categorical\" variables:\n",
    "dummies_cat_v2 = pd.get_dummies(X_cat_v2, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating both \"X_num\" and \"X_cat\" variables:\n",
    "X_v2 = np.concatenate((scaler_num_v2, dummies_cat_v2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data between \"train\" and \"test\":\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_v2, y_v2, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Simple)_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Simple):\n",
      "MSE (Mean Squared Error): 1813.5374\n",
      "R2 (Coefficient of Determination): 0.0328\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fitting the model:\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lm = lm.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "r2_lm = r2_score(y_test, y_pred_lm)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Simple):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lm:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Ridge)_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Ridge's Best Parameters: {'alpha': 10, 'max_iter': 10000, 'solver': 'sag'}\n",
      "Ridge's Best Score: 0.03137399208245171\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"solver\": [\"auto\", \"sag\", \"lsqr\"],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=Ridge(),     # Model we want to fit\n",
    "    param_grid=param_grid_ridge,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Ridge's Best Parameters:\", grid_search_ridge.best_params_)\n",
    "print(\"Ridge's Best Score:\", grid_search_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Ridge):\n",
      "MSE (Mean Squared Error): 1813.5590\n",
      "R2 (Coefficient of Determination): 0.0328\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "ridge = Ridge(alpha=10, max_iter=1000, solver=\"sag\")\n",
    "\n",
    "# Fitting the model:\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Ridge):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_ridge:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Lasso)_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Lasso's Best Parameters: {'alpha': 0.001, 'max_iter': 1000, 'tol': 1e-06}\n",
      "Lasso's Best Score: 0.03137470788644492\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_lasso = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"tol\": [1e-4, 1e-5, 1e-6],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=Lasso(),     # Model we want to fit\n",
    "    param_grid=param_grid_lasso,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Lasso's Best Parameters:\", grid_search_lasso.best_params_)\n",
    "print(\"Lasso's Best Score:\", grid_search_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Lasso):\n",
      "MSE (Mean Squared Error): 1813.5546\n",
      "R2 (Coefficient of Determination): 0.0328\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "lasso = Lasso(alpha=0.001, max_iter=1000, tol=0.000001)\n",
    "\n",
    "# Fitting the model:\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Lasso):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lasso:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Support Vector Regression:\n",
      "MSE (Mean Squared Error): 1845.1639\n",
      "R2 (Coefficient of Determination): 0.0160\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "svr = SVR(C=100, epsilon=0.5, kernel=\"rbf\")\n",
    "\n",
    "# Fitting the model:\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Support Vector Regression:\")\n",
    "print(f'MSE (Mean Squared Error): {mse_svr:.4f}')\n",
    "print(f'R2 (Coefficient of Determination): {r2_svr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Random Forest Regression:\n",
      "MSE (Mean Squared Error): 1774.6060\n",
      "R2 (Coefficient of Determination): 0.0536\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=4, random_state=42) \n",
    "\n",
    "# Fitting the model:\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_rfr = rfr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_rfr = mean_squared_error(y_test, y_pred_rfr)\n",
    "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Random Forest Regression:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_rfr:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_rfr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a different approach..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_filtered = wines_dataset[wines_dataset[\"price_usd\"]<=75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_filtered = np.percentile(wines_filtered[\"vintage\"], 75)-np.percentile(wines_filtered[\"vintage\"], 25)\n",
    "lower_limit_filtered = np.percentile(wines_filtered[\"vintage\"], 25)-3*iqr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_predictor_filtered = wines_filtered[wines_filtered[\"vintage\"] >= lower_limit_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dependent variable:\n",
    "y_v3 = wines_predictor_filtered[\"price_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables:\n",
    "X_v3 = wines_predictor_filtered.drop(columns=[\"price_usd\"])\n",
    "\n",
    "X_num_v3 = X_v3.select_dtypes(np.number)     # It takes all \"numerical\" variables\n",
    "X_cat_v3 = X_v3.select_dtypes(object)     # It takes all \"categorical\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \"numerical\" variables:\n",
    "scaler_num_v3 = MinMaxScaler().fit_transform(X_num_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for \"categorical\" variables:\n",
    "dummies_cat_v3 = pd.get_dummies(X_cat_v3, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating both \"X_num\" and \"X_cat\" variables:\n",
    "X_v3 = np.concatenate((scaler_num_v3, dummies_cat_v3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data between \"train\" and \"test\":\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_v3, y_v3, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Simple)_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Simple):\n",
      "MSE (Mean Squared Error): 160.0123\n",
      "R2 (Coefficient of Determination): 0.3869\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fitting the model:\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lm = lm.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "r2_lm = r2_score(y_test, y_pred_lm)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Simple):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lm:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Ridge)_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Ridge's Best Parameters: {'alpha': 0.1, 'max_iter': 10000, 'solver': 'sag'}\n",
      "Ridge's Best Score: 0.3779636691090883\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"solver\": [\"auto\", \"sag\", \"lsqr\"],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=Ridge(),     # Model we want to fit\n",
    "    param_grid=param_grid_ridge,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Ridge's Best Parameters:\", grid_search_ridge.best_params_)\n",
    "print(\"Ridge's Best Score:\", grid_search_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Ridge):\n",
      "MSE (Mean Squared Error): 160.0136\n",
      "R2 (Coefficient of Determination): 0.3869\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "ridge = Ridge(alpha=1, max_iter=1000, solver=\"sag\")\n",
    "\n",
    "# Fitting the model:\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Ridge):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_ridge:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Lasso)_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Lasso's Best Parameters: {'alpha': 0.0001, 'max_iter': 1000, 'tol': 1e-06}\n",
      "Lasso's Best Score: 0.3779631483107007\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_lasso = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"tol\": [1e-4, 1e-5, 1e-6],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=Lasso(),     # Model we want to fit\n",
    "    param_grid=param_grid_lasso,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Lasso's Best Parameters:\", grid_search_lasso.best_params_)\n",
    "print(\"Lasso's Best Score:\", grid_search_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Lasso):\n",
      "MSE (Mean Squared Error): 160.0126\n",
      "R2 (Coefficient of Determination): 0.3869\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "lasso = Lasso(alpha=0.0001, max_iter=1000, tol=0.000001)\n",
    "\n",
    "# Fitting the model:\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Lasso):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lasso:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Support Vector Regression:\n",
      "MSE (Mean Squared Error): 146.1026\n",
      "R2 (Coefficient of Determination): 0.4402\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "svr = SVR(C=100, epsilon=0.5, kernel=\"rbf\")\n",
    "\n",
    "# Fitting the model:\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Support Vector Regression:\")\n",
    "print(f'MSE (Mean Squared Error): {mse_svr:.4f}')\n",
    "print(f'R2 (Coefficient of Determination): {r2_svr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Random Forest Regression:\n",
      "MSE (Mean Squared Error): 141.4852\n",
      "R2 (Coefficient of Determination): 0.4579\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=4, random_state=42) \n",
    "\n",
    "# Fitting the model:\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_rfr = rfr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_rfr = mean_squared_error(y_test, y_pred_rfr)\n",
    "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Random Forest Regression:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_rfr:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_rfr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a different approach_v2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_filtered_v2 = wines_dataset[wines_dataset[\"price_usd\"]<=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_filtered_v2 = np.percentile(wines_filtered_v2[\"vintage\"], 75)-np.percentile(wines_filtered_v2[\"vintage\"], 25)\n",
    "lower_limit_filtered_v2 = np.percentile(wines_filtered_v2[\"vintage\"], 25)-3*iqr_filtered_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines_predictor_filtered_v2 = wines_filtered_v2[wines_filtered_v2[\"vintage\"] >= lower_limit_filtered_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dependent variable:\n",
    "y_v4 = wines_predictor_filtered_v2[\"price_usd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the independent variables:\n",
    "X_v4 = wines_predictor_filtered_v2.drop(columns=[\"price_usd\"])\n",
    "\n",
    "X_num_v4 = X_v4.select_dtypes(np.number)     # It takes all \"numerical\" variables\n",
    "X_cat_v4 = X_v4.select_dtypes(object)     # It takes all \"categorical\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling \"numerical\" variables:\n",
    "scaler_num_v4 = MinMaxScaler().fit_transform(X_num_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for \"categorical\" variables:\n",
    "dummies_cat_v4 = pd.get_dummies(X_cat_v4, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating both \"X_num\" and \"X_cat\" variables:\n",
    "X_v4 = np.concatenate((scaler_num_v4, dummies_cat_v4), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data between \"train\" and \"test\":\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_v4, y_v4, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Simple)_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Simple):\n",
      "MSE (Mean Squared Error): 86.8623\n",
      "R2 (Coefficient of Determination): 0.3287\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fitting the model:\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lm = lm.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lm = mean_squared_error(y_test, y_pred_lm)\n",
    "r2_lm = r2_score(y_test, y_pred_lm)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Simple):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lm:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Ridge)_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Ridge's Best Parameters: {'alpha': 0.01, 'max_iter': 5000, 'solver': 'sag'}\n",
      "Ridge's Best Score: 0.33413331962796\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"solver\": [\"auto\", \"sag\", \"lsqr\"],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=Ridge(),     # Model we want to fit\n",
    "    param_grid=param_grid_ridge,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Ridge's Best Parameters:\", grid_search_ridge.best_params_)\n",
    "print(\"Ridge's Best Score:\", grid_search_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Ridge):\n",
      "MSE (Mean Squared Error): 86.8623\n",
      "R2 (Coefficient of Determination): 0.3287\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "ridge = Ridge(alpha=0.01, max_iter=5000, solver=\"sag\")\n",
    "\n",
    "# Fitting the model:\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Ridge):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_ridge:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Lasso)_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Lasso's Best Parameters: {'alpha': 0.0001, 'max_iter': 1000, 'tol': 1e-06}\n",
      "Lasso's Best Score: 0.3341317557187241\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter grid (check the model's documentation to know the exact names of the hyperparameters):\n",
    "param_grid_lasso = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    \"tol\": [1e-4, 1e-5, 1e-6],\n",
    "    \"max_iter\": [1000, 5000, 10000]}\n",
    "\n",
    "# Creating GridSearchCV:\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=Lasso(),     # Model we want to fit\n",
    "    param_grid=param_grid_lasso,     # Hyperparameters grid\n",
    "    cv=5,     # Number of Cross-Validation folds\n",
    "    scoring=\"r2\",     # Scoring method we'll use to determine the best hyperparameters\n",
    "\tverbose=1)     # It prints the results at each step\n",
    "\n",
    "# Performing Grid Search:\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Returning Best parameters & Best score:\n",
    "print(\"Lasso's Best Parameters:\", grid_search_lasso.best_params_)\n",
    "print(\"Lasso's Best Score:\", grid_search_lasso.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Linear Regression (Lasso):\n",
      "MSE (Mean Squared Error): 86.8612\n",
      "R2 (Coefficient of Determination): 0.3287\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "lasso = Lasso(alpha=0.0001, max_iter=1000, tol=0.000001)\n",
    "\n",
    "# Fitting the model:\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Linear Regression (Lasso):\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_lasso:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Support Vector Regression:\n",
      "MSE (Mean Squared Error): 79.9075\n",
      "R2 (Coefficient of Determination): 0.3824\n"
     ]
    }
   ],
   "source": [
    "# Defining the model with the best parameters:\n",
    "svr = SVR(C=100, epsilon=0.5, kernel=\"rbf\")\n",
    "\n",
    "# Fitting the model:\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Support Vector Regression:\")\n",
    "print(f'MSE (Mean Squared Error): {mse_svr:.4f}')\n",
    "print(f'R2 (Coefficient of Determination): {r2_svr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Random Forest Regression:\n",
      "MSE (Mean Squared Error): 77.8404\n",
      "R2 (Coefficient of Determination): 0.3984\n"
     ]
    }
   ],
   "source": [
    "# Defining the model:\n",
    "rfr = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_leaf=4, random_state=42) \n",
    "\n",
    "# Fitting the model:\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions:\n",
    "y_pred_rfr = rfr.predict(X_test)\n",
    "\n",
    "# Evaluating performance of the model:\n",
    "mse_rfr = mean_squared_error(y_test, y_pred_rfr)\n",
    "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
    "\n",
    "# Printing the performance's results:\n",
    "print(\"📘 Random Forest Regression:\")\n",
    "print(f\"MSE (Mean Squared Error): {mse_rfr:.4f}\")\n",
    "print(f\"R2 (Coefficient of Determination): {r2_rfr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Original",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
